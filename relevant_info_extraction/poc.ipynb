{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f17458ac-6bcf-4574-bb5e-320e1c8a44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf8877-7c83-4efe-a17d-68bd6671424a",
   "metadata": {},
   "source": [
    "### Rows and columns with maximum number of non-zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6db52a56-7694-4df6-8f09-d5e8bee94f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with maximum number of non-zero values:\n",
      "['USA' '331,449,281' '9,826,630' '62,794']\n",
      "['Canada' '38,048,738' '9,984,670' '48,265']\n",
      "['Mexico' '129,166,028' '1,964,375' '9,747']\n",
      "['Argentina' '45,197,956' '2,780,400' '11,866']\n",
      "['Brazil' '212,559,417' '8,515,767' '9,126']\n",
      "Columns with maximum number of non-zero values:\n",
      "['Country' 'USA' 'Canada' 'Mexico' 'Argentina' 'Brazil']\n",
      "['Population' '331,449,281' '38,048,738' '129,166,028' '45,197,956'\n",
      " '212,559,417']\n",
      "['Area (km²)' '9,826,630' '9,984,670' '1,964,375' '2,780,400' '8,515,767']\n",
      "['GDP per capita (USD)' '62,794' '48,265' '9,747' '11,866' '9,126']\n"
     ]
    }
   ],
   "source": [
    "table = np.array([\n",
    "    [ 'Country', 'Population', 'Area (km²)', 'GDP per capita (USD)'],\n",
    "    [ 'USA', '331,449,281', '9,826,630', '62,794'],\n",
    "    [ 'Canada', '38,048,738', '9,984,670', '48,265'],\n",
    "    [ 'Mexico', '129,166,028', '1,964,375', '9,747'],\n",
    "    [ 'Argentina', '45,197,956', '2,780,400', '11,866'],\n",
    "    [ 'Brazil', '212,559,417', '8,515,767', '9,126']\n",
    "])\n",
    "\n",
    "non_zero_counts = np.count_nonzero(table[1:], axis=1)\n",
    "max_non_zero_count = np.max(non_zero_counts)\n",
    "max_non_zero_rows = table[1:][non_zero_counts == max_non_zero_count]\n",
    "\n",
    "print(\"Rows with maximum number of non-zero values:\")\n",
    "for row in max_non_zero_rows:\n",
    "    print(row)\n",
    "\n",
    "non_zero_counts = np.count_nonzero(table[1:], axis=0)\n",
    "max_non_zero_count = np.max(non_zero_counts)\n",
    "max_non_zero_cols = table[:,non_zero_counts == max_non_zero_count]\n",
    "\n",
    "print(\"Columns with maximum number of non-zero values:\")\n",
    "for col in max_non_zero_cols.T:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139b65e-1c88-47cf-b73c-ca674f118a37",
   "metadata": {},
   "source": [
    "### Rows with the most frequent values as relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f75dcaf-cb6e-4d4c-92d6-d2708ab30639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Country  Population  Area (km²)  GDP per capita (USD)\n",
      "1     Canada    38048738     9984670                 48265\n",
      "2     Mexico   129166028     1964375                  9747\n",
      "3  Argentina    45197956     2780400                 11866\n",
      "4     Brazil   212559417     8515767                  9126\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"Country\": [\"USA\", \"Canada\", \"Mexico\", \"Argentina\", \"Brazil\"],\n",
    "    \"Population\": [331449281, 38048738, 129166028, 45197956, 212559417],\n",
    "    \"Area (km²)\": [9826630, 9984670, 1964375, 2780400, 8515767],\n",
    "    \"GDP per capita (USD)\": [62794, 48265, 9747, 11866, 9126]\n",
    "}\n",
    "table = pd.DataFrame(data)\n",
    "\n",
    "most_frequent_row = table.mode().iloc[0]\n",
    "\n",
    "relevant_rows = table[table.eq(most_frequent_row).any(axis=1)]\n",
    "\n",
    "print(relevant_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a1a92-5203-461a-98f0-f8295fee2038",
   "metadata": {},
   "source": [
    "### Columns with the highest standard deviation as relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d6b8ddd-d561-45ce-a26b-3954a15c14d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Population\n",
      "0   331449281\n",
      "1    38048738\n",
      "2   129166028\n",
      "3    45197956\n",
      "4   212559417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_371043/2781234655.py:9: FutureWarning: The default value of numeric_only in DataFrame.std is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  highest_std_col = table.std().idxmax()\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"Country\": [\"USA\", \"Canada\", \"Mexico\", \"Argentina\", \"Brazil\"],\n",
    "    \"Population\": [331449281, 38048738, 129166028, 45197956, 212559417],\n",
    "    \"Area (km²)\": [9826630, 9984670, 1964375, 2780400, 8515767],\n",
    "    \"GDP per capita (USD)\": [62794, 48265, 9747, 11866, 9126]\n",
    "}\n",
    "table = pd.DataFrame(data)\n",
    "\n",
    "highest_std_col = table.std().idxmax()\n",
    "\n",
    "relevant_cols = table.loc[:, [highest_std_col]]\n",
    "\n",
    "print(relevant_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa10c50-d806-4f68-a7a3-7f4754cf1812",
   "metadata": {},
   "source": [
    "### Columns with a high percentage of non-empty cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4aad71eb-5f6d-493b-8c89-6936bbf9ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Country  Population  Area (km²)  GDP per capita (USD)\n",
      "0        USA   331449281     9826630                 62794\n",
      "1     Canada    38048738     9984670                 48265\n",
      "2     Mexico   129166028     1964375                  9747\n",
      "3  Argentina    45197956     2780400                 11866\n",
      "4     Brazil   212559417     8515767                  9126\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"Country\": [\"USA\", \"Canada\", \"Mexico\", \"Argentina\", \"Brazil\"],\n",
    "    \"Population\": [331449281, 38048738, 129166028, 45197956, 212559417],\n",
    "    \"Area (km²)\": [9826630, 9984670, 1964375, 2780400, 8515767],\n",
    "    \"GDP per capita (USD)\": [62794, 48265, 9747, 11866, 9126]\n",
    "}\n",
    "table = pd.DataFrame(data)\n",
    "\n",
    "non_empty_percent = table.count() / table.shape[0] * 100\n",
    "\n",
    "relevant_cols = table.loc[:, non_empty_percent > 50]\n",
    "\n",
    "print(relevant_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b78661-2ad8-4ec6-9be3-7d68bb85c613",
   "metadata": {},
   "source": [
    "### Rows that contain aggregate data such as sums, averages, etc. can also be considered important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2391b77e-c671-4067-b577-5c69af63de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          City  Population  Median Age  Average Income (USD)  \\\n",
      "0     New York     8336817        36.4                 77000   \n",
      "1  Los Angeles     3979576        35.2                 65000   \n",
      "2      Chicago     2693976        34.1                 63000   \n",
      "3      Houston     2320268        33.8                 59000   \n",
      "4      Phoenix     1680992        33.6                 55000   \n",
      "\n",
      "   Average Rent (USD)  Mean Population  Sum Avg Income and Rent  \n",
      "0                2900      2104188.350                    79900  \n",
      "1                2300      1011727.800                    67300  \n",
      "2                1800       689702.525                    64800  \n",
      "3                1200       595125.450                    60200  \n",
      "4                1100       434281.400                    56100  \n"
     ]
    }
   ],
   "source": [
    "data = {'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n",
    "        'Population': [8336817, 3979576, 2693976, 2320268, 1680992],\n",
    "        'Median Age': [36.4, 35.2, 34.1, 33.8, 33.6],\n",
    "        'Average Income (USD)': [77000, 65000, 63000, 59000, 55000],\n",
    "        'Average Rent (USD)': [2900, 2300, 1800, 1200, 1100]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "agg_cols = ['Population', 'Median Age', 'Average Income (USD)', 'Average Rent (USD)']\n",
    "\n",
    "df['Mean Population'] = df[agg_cols].mean(axis=1)\n",
    "df['Sum Avg Income and Rent'] = df['Average Income (USD)'] + df['Average Rent (USD)']\n",
    "\n",
    "sorted_df = df.sort_values(by=['Mean Population', 'Sum Avg Income and Rent'], ascending=[False, False])\n",
    "\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1075a2-589a-45f6-ab04-cfa5479b1545",
   "metadata": {},
   "source": [
    "### Correlation: Relevant rows and columns can be selected based on the correlation of the data in a column with the data in other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124c41b-96ce-4120-ab22-3ddfc42d142b",
   "metadata": {},
   "source": [
    "| City      | Population | Area (km²) | Average temperature (°C) | Elevation (m) |\n",
    "|-----------|------------|------------|--------------------------|---------------|\n",
    "| Tokyo     | 13,515,271 | 2,187      | 16.2                     | 44            |\n",
    "| New York  | 8,175,133  | 468.9      | 12.9                     | 10            |\n",
    "| London    | 8,982,000  | 1,572      | 9.8                      | 35            |\n",
    "| Paris     | 2,148,271  | 105.4      | 11.5                     | 35            |\n",
    "| Istanbul  | 15,029,231 | 5,461      | 13.9                     | 110           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "645f9104-7eaa-4e0e-bc8f-88cc72e86752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Population  Area (km²)  Average temperature (°C)  Elevation (m)\n",
      "0    13515271      2187.0                      16.2             44\n",
      "1     8175133       468.9                      12.9             10\n",
      "2     8982000      1572.0                       9.8             35\n",
      "3     2148271       105.4                      11.5             35\n",
      "4    15029231      5461.0                      13.9            110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_371043/3838197159.py:8: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  corr_matrix = df.corr()\n"
     ]
    }
   ],
   "source": [
    "data = {'City': ['Tokyo', 'New York', 'London', 'Paris', 'Istanbul'],\n",
    "        'Population': [13515271, 8175133, 8982000, 2148271, 15029231],\n",
    "        'Area (km²)': [2187, 468.9, 1572, 105.4, 5461],\n",
    "        'Average temperature (°C)': [16.2, 12.9, 9.8, 11.5, 13.9],\n",
    "        'Elevation (m)': [44, 10, 35, 35, 110]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "relevant_columns = []\n",
    "threshold = 0.8\n",
    "for col in corr_matrix.columns:\n",
    "    if max(corr_matrix[col]) > threshold:\n",
    "        relevant_columns.append(col)\n",
    "\n",
    "df_relevant = df[relevant_columns]\n",
    "\n",
    "print(df_relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043c2e1-0853-4ce4-8269-945986e71ddc",
   "metadata": {},
   "source": [
    "### Specific pattern match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "031d080a-f25b-4ab0-b6d8-646d3ab96f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['John Smith', '32', 'New York City'], ['Jane Doe', '25', 'Los Angeles'], ['Bob Johnson', '43', 'Chicago']]\n"
     ]
    }
   ],
   "source": [
    "table = [\n",
    "    [\"Name\", \"Age\", \"City\"],\n",
    "    [\"John Smith\", \"32\", \"New York City\"],\n",
    "    [\"Jane Doe\", \"25\", \"Los Angeles\"],\n",
    "    [\"Bob Johnson\", \"43\", \"Chicago\"],\n",
    "    [\"Alice Williams\", \"29\", \"San Francisco\"]\n",
    "]\n",
    "\n",
    "city_regex = re.compile(r\"^(New York City|Los Angeles|Chicago)$\")\n",
    "\n",
    "city_col_index = None\n",
    "for i, col in enumerate(table[0]):\n",
    "    if col == \"City\":\n",
    "        city_col_index = i\n",
    "        break\n",
    "\n",
    "selected_rows = []\n",
    "for row in table[1:]:\n",
    "    if city_regex.match(row[city_col_index]):\n",
    "        selected_rows.append(row)\n",
    "\n",
    "print(selected_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7024c1-02f8-4e7c-9087-93bd0be1ea08",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794388ba-d559-46b0-a109-e991da517d77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Rows:\n",
      "['Apples', 'Fresh, juicy apples from our orchard', '$0.99']\n",
      "['Bananas', 'Ripe, yellow bananas from Ecuador', '$0.69']\n",
      "['Oranges', 'Sweet and tangy Florida oranges', '$1.25']\n",
      "['Grapes', 'Plump grapes from California vineyards', '$1.49']\n",
      "['Strawberries', 'Fresh strawberries from local farms', '$2.99']\n",
      "['Blueberries', 'Juicy blueberries from Maine', '$3.99']\n",
      "['Raspberries', 'Sweet and tart raspberries from Oregon', '$4.99']\n",
      "['Peaches', 'Juicy, ripe peaches from Georgia', '$1.99']\n",
      "\n",
      "Relevant Columns:\n",
      "['Description', 'Fresh, juicy apples from our orchard', 'Ripe, yellow bananas from Ecuador', 'Sweet and tangy Florida oranges', 'Plump grapes from California vineyards', 'Fresh strawberries from local farms', 'Juicy blueberries from Maine', 'Sweet and tart raspberries from Oregon', 'Juicy, ripe peaches from Georgia']\n",
      "['Product', 'Apples', 'Bananas', 'Oranges', 'Grapes', 'Strawberries', 'Blueberries', 'Raspberries', 'Peaches']\n",
      "['Description', 'Fresh, juicy apples from our orchard', 'Ripe, yellow bananas from Ecuador', 'Sweet and tangy Florida oranges', 'Plump grapes from California vineyards', 'Fresh strawberries from local farms', 'Juicy blueberries from Maine', 'Sweet and tart raspberries from Oregon', 'Juicy, ripe peaches from Georgia']\n",
      "['Description', 'Fresh, juicy apples from our orchard', 'Ripe, yellow bananas from Ecuador', 'Sweet and tangy Florida oranges', 'Plump grapes from California vineyards', 'Fresh strawberries from local farms', 'Juicy blueberries from Maine', 'Sweet and tart raspberries from Oregon', 'Juicy, ripe peaches from Georgia']\n",
      "['Description', 'Fresh, juicy apples from our orchard', 'Ripe, yellow bananas from Ecuador', 'Sweet and tangy Florida oranges', 'Plump grapes from California vineyards', 'Fresh strawberries from local farms', 'Juicy blueberries from Maine', 'Sweet and tart raspberries from Oregon', 'Juicy, ripe peaches from Georgia']\n",
      "['Description', 'Fresh, juicy apples from our orchard', 'Ripe, yellow bananas from Ecuador', 'Sweet and tangy Florida oranges', 'Plump grapes from California vineyards', 'Fresh strawberries from local farms', 'Juicy blueberries from Maine', 'Sweet and tart raspberries from Oregon', 'Juicy, ripe peaches from Georgia']\n",
      "['Description', 'Fresh, juicy apples from our orchard', 'Ripe, yellow bananas from Ecuador', 'Sweet and tangy Florida oranges', 'Plump grapes from California vineyards', 'Fresh strawberries from local farms', 'Juicy blueberries from Maine', 'Sweet and tart raspberries from Oregon', 'Juicy, ripe peaches from Georgia']\n",
      "['Description', 'Fresh, juicy apples from our orchard', 'Ripe, yellow bananas from Ecuador', 'Sweet and tangy Florida oranges', 'Plump grapes from California vineyards', 'Fresh strawberries from local farms', 'Juicy blueberries from Maine', 'Sweet and tart raspberries from Oregon', 'Juicy, ripe peaches from Georgia']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "table_data = [\n",
    "    [\"Product\", \"Description\", \"Price\"],\n",
    "    [\"Apples\", \"Fresh, juicy apples from our orchard\", \"$0.99\"],\n",
    "    [\"Bananas\", \"Ripe, yellow bananas from Ecuador\", \"$0.69\"],\n",
    "    [\"Oranges\", \"Sweet and tangy Florida oranges\", \"$1.25\"],\n",
    "    [\"Grapes\", \"Plump grapes from California vineyards\", \"$1.49\"],\n",
    "    [\"Strawberries\", \"Fresh strawberries from local farms\", \"$2.99\"],\n",
    "    [\"Blueberries\", \"Juicy blueberries from Maine\", \"$3.99\"],\n",
    "    [\"Raspberries\", \"Sweet and tart raspberries from Oregon\", \"$4.99\"],\n",
    "    [\"Peaches\", \"Juicy, ripe peaches from Georgia\", \"$1.99\"],\n",
    "]\n",
    "\n",
    "relevant_rows = []\n",
    "relevant_cols = []\n",
    "\n",
    "for row in table_data:\n",
    "    relevant_flag = False\n",
    "    \n",
    "    for cell in row:\n",
    "        doc = nlp(cell)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        if any(ent[1] == \"GPE\" for ent in entities):\n",
    "            relevant_flag = True\n",
    "            break\n",
    "        \n",
    "        blob = TextBlob(cell)\n",
    "        sentiment = blob.sentiment.polarity\n",
    "        \n",
    "        if sentiment > 0:\n",
    "            relevant_flag = True\n",
    "            break\n",
    "    \n",
    "    if relevant_flag:\n",
    "        relevant_rows.append(row)\n",
    "    \n",
    "    for i, cell in enumerate(row):\n",
    "        doc = nlp(cell)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        if any(ent[1] == \"GPE\" for ent in entities):\n",
    "            relevant_cols.append(i)\n",
    "            break\n",
    "        \n",
    "        blob = TextBlob(cell)\n",
    "        sentiment = blob.sentiment.polarity\n",
    "        \n",
    "        if sentiment > 0:\n",
    "            relevant_cols.append(i)\n",
    "            break\n",
    "\n",
    "print(\"Relevant Rows:\")\n",
    "for row in relevant_rows:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nRelevant Columns:\")\n",
    "for col in relevant_cols:\n",
    "    column_data = [row[col] for row in table_data]\n",
    "    print(column_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5addcac6-c48b-4cf3-8de1-bd2a35fdd726",
   "metadata": {
    "tags": []
   },
   "source": [
    "### very basic and similar to https://arxiv.org/pdf/2005.09207.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ce4eeea-f36e-4718-8ca2-ada60b53f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "685922f1-9717-4fca-9430-f875bca62a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_table_query(model, table, query):\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    inputs = tokenizer.encode_plus(table, query, add_special_tokens=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4388dce8-756e-4697-824e-234435b6d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_info(table_representation, query_representation):\n",
    "    scores = torch.nn.functional.cosine_similarity(table_representation, query_representation)\n",
    "    relevant_rows = scores.topk(k=1, dim=0).indices.tolist()\n",
    "    return relevant_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35f300be-e16b-4192-99b1-65098eb4e99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant rows: [0]\n"
     ]
    }
   ],
   "source": [
    "table = \"Name | Capital | Population\\n------------------------------\\nFrance| Paris  | 66 million\\n------------------------------\\nGermany| Berlin | 83 million\\n------------------------------\\nItaly | Rome   | 60 million\"\n",
    "query = \"Capital and population of Cities?\"\n",
    "\n",
    "table_representation = encode_table_query(model, table, \"\")\n",
    "query_representation = encode_table_query(model, \"\", query)\n",
    "relevant_rows = retrieve_relevant_info(table_representation, query_representation)\n",
    "\n",
    "print(\"Relevant rows:\", relevant_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa67b8f0-d6ba-45b2-8847-ce095d34dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_table(model, table, caption, reference):\n",
    "    input_text = caption + \" \" + reference + \" \" + table\n",
    "    input_ids = transformers.BertTokenizer.from_pretrained('bert-base-uncased').encode(input_text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    return outputs[0].mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "728016d0-8bca-49fd-b846-36874634bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevancePredictor(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(RelevancePredictor, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39c64957-41ed-499a-88c8-4c62259f2d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance predictions: tensor([[-0.2970]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "table = \"Name | Capital | Population\\n------------------------------\\nFrance| Paris  | 66 million\\n------------------------------\\nGermany| Berlin | 83 million\\n------------------------------\\nItaly | Rome   | 60 million\"\n",
    "caption = \"Information about countries\"\n",
    "reference = \"Source: Wikipedia\"\n",
    "\n",
    "table_representation = encode_table(model, table, caption, reference)\n",
    "model = RelevancePredictor(hidden_size=table_representation.size(-1), num_classes=1)\n",
    "predictions = model(table_representation)\n",
    "\n",
    "print(\"Relevance predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0201c00-c52d-48c7-a790-fa13965003ca",
   "metadata": {},
   "source": [
    "### Basic Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e628708-180e-4b22-b3a4-90bad4e02c98",
   "metadata": {},
   "source": [
    "| Fruit   | Color  | Price |\n",
    "|---------|--------|-------|\n",
    "| Apple   | Red    | 0.99  |\n",
    "| Orange  | Orange | 1.25  |\n",
    "| Banana  | Yellow | 0.69  |\n",
    "| Kiwi    | Brown  | 1.50  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4612a1c-e526-4b03-a05e-7b6c0058b946",
   "metadata": {},
   "source": [
    "[CLS] Fruit [SEP] Color [SEP] Price [SEP]\n",
    "Apple [SEP] Red [SEP] 0.99 [SEP]\n",
    "Orange [SEP] Orange [SEP] 1.25 [SEP]\n",
    "Banana [SEP] Yellow [SEP] 0.69 [SEP]\n",
    "Kiwi [SEP] Brown [SEP] 1.50 [SEP]\n",
    "[SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1b12f-1502-4cd5-a2c6-2198fa5c6b8d",
   "metadata": {},
   "source": [
    "### Encoding table information ... Similar to TaBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4c99aa-1397-41bf-bf73-27f9ec25877e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 1.22MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e9a714c-44b6-4250-a8ad-822f6d734644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_data = [\n",
    "    [\"Name\", \"Age\", \"Gender\"],\n",
    "    [\"Alice\", \"25\", \"Female\"],\n",
    "    [\"Bob\", \"30\", \"Male\"],\n",
    "    [\"Carol\", \"40\", \"Female\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6258ea7d-b2fd-4c12-bd25-e88dd10c9350",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2171,  101, 1031, 1014, 1033, 1031, 1014, 1033,  102],\n",
      "        [2287,  101, 1031, 1014, 1033, 1031, 1015, 1033,  102],\n",
      "        [5907,  101, 1031, 1014, 1033, 1031, 1016, 1033,  102],\n",
      "        [2198,  101, 1031, 1015, 1033, 1031, 1014, 1033,  102],\n",
      "        [2423,  101, 1031, 1015, 1033, 1031, 1015, 1033,  102],\n",
      "        [3287,  101, 1031, 1015, 1033, 1031, 1016, 1033,  102],\n",
      "        [4532,  101, 1031, 1016, 1033, 1031, 1014, 1033,  102],\n",
      "        [3590,  101, 1031, 1016, 1033, 1031, 1015, 1033,  102],\n",
      "        [2931,  101, 1031, 1016, 1033, 1031, 1016, 1033,  102],\n",
      "        [3419,  101, 1031, 1017, 1033, 1031, 1014, 1033,  102],\n",
      "        [2539,  101, 1031, 1017, 1033, 1031, 1015, 1033,  102],\n",
      "        [3287,  101, 1031, 1017, 1033, 1031, 1016, 1033,  102]])\n"
     ]
    }
   ],
   "source": [
    "cells = []\n",
    "\n",
    "for i, row in enumerate(table):\n",
    "    for j, cell in enumerate(row):\n",
    "        cell_tokens = tokenizer.encode(cell, add_special_tokens=False)\n",
    "        position_tokens = tokenizer.encode(f'[{i}][{j}]')\n",
    "        cells.append(cell_tokens + position_tokens)\n",
    "\n",
    "max_len = max(len(cell) for cell in cells)\n",
    "padded_cells = [cell + [tokenizer.pad_token_id] * (max_len - len(cell)) for cell in cells]\n",
    "input_ids = torch.tensor(padded_cells)\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "247e76f0-e299-4cd7-9334-68bd497550d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2171,  102, 2171,  101, 1031, 1015, 1033, 1031, 1014, 1033,  102],\n",
      "        [2287,  102, 2287,  101, 1031, 1015, 1033, 1031, 1015, 1033,  102],\n",
      "        [5907,  102, 5907,  101, 1031, 1015, 1033, 1031, 1016, 1033,  102],\n",
      "        [2198,  101, 1031, 1016, 1033, 1031, 1014, 1033,  102,    0,    0],\n",
      "        [2423,  101, 1031, 1016, 1033, 1031, 1015, 1033,  102,    0,    0],\n",
      "        [3287,  101, 1031, 1016, 1033, 1031, 1016, 1033,  102,    0,    0],\n",
      "        [4532,  101, 1031, 1017, 1033, 1031, 1014, 1033,  102,    0,    0],\n",
      "        [3590,  101, 1031, 1017, 1033, 1031, 1015, 1033,  102,    0,    0],\n",
      "        [2931,  101, 1031, 1017, 1033, 1031, 1016, 1033,  102,    0,    0],\n",
      "        [3419,  101, 1031, 1018, 1033, 1031, 1014, 1033,  102,    0,    0],\n",
      "        [2539,  101, 1031, 1018, 1033, 1031, 1015, 1033,  102,    0,    0],\n",
      "        [3287,  101, 1031, 1018, 1033, 1031, 1016, 1033,  102,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "cells = []\n",
    "for i, row in enumerate(table):\n",
    "    for j, cell in enumerate(row):\n",
    "        cell_tokens = tokenizer.encode(cell, add_special_tokens=False)\n",
    "        position_tokens = tokenizer.encode(f'[{i+1}] [{j}]')\n",
    "        if i == 0:\n",
    "            header_tokens = tokenizer.encode(cell, add_special_tokens=False)\n",
    "            cells.append(header_tokens + [tokenizer.sep_token_id] + cell_tokens + position_tokens)\n",
    "        else:\n",
    "            cells.append(cell_tokens + position_tokens)\n",
    "\n",
    "max_len = max(len(cell) for cell in cells)\n",
    "padded_cells = [cell + [tokenizer.pad_token_id] * (max_len - len(cell)) for cell in cells]\n",
    "input_ids = torch.tensor(padded_cells)\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dade32-aa95-4ea6-9330-b229e41c40b5",
   "metadata": {},
   "source": [
    "### Using statistical measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0509552-3d11-4a85-8a56-5294f236c8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Population    GDP Correlation P-Value\n",
      "Country                                       \n",
      "USA             331  21428         NaN     NaN\n",
      "China          1393  14732         NaN     NaN\n",
      "India          1380   2955         NaN     NaN\n",
      "Russia          144   1678         NaN     NaN\n",
      "Japan           126   5068         NaN     NaN\n"
     ]
    }
   ],
   "source": [
    "# In this example, I am selecting the relevant cells based on their correlation with GDP.\n",
    "# In this example, I have selected the relevant cells (Population and GDP) based on their correlation with GDP.\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Country': ['USA', 'China', 'India', 'Russia', 'Japan'],\n",
    "    'Population': [331, 1393, 1380, 144, 126],\n",
    "    'GDP': [21428, 14732, 2955, 1678, 5068],\n",
    "    'Life Expectancy': [78.9, 76.9, 69.7, 72.3, 84.6]\n",
    "})\n",
    "data = data.set_index('Country')\n",
    "\n",
    "# select relevant cells based on correlation with GDP\n",
    "relevant_columns = ['Population', 'GDP']\n",
    "correlations = data[relevant_columns].apply(lambda x: pearsonr(x, data['GDP']))\n",
    "correlations_df = pd.DataFrame({'Correlation': [corr[0] for corr in correlations], 'P-Value': [corr[1] for corr in correlations]}, index=relevant_columns)\n",
    "relevant_data = data[relevant_columns]\n",
    "\n",
    "print(relevant_data.join(correlations_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc1fcc4-491d-4964-918e-c4655318e101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this example, I am selecting the relevant cells based on k-means clustering and linear regression. \n",
    "# to cluster the data into two clusters based on the Population and GDP columns. \n",
    "# I have then selected the cells from the second cluster as they have a higher GDP. \n",
    "# I have also used LinearRegression to fit a linear regression model to predict the GDP based on the population.\n",
    "\n",
    "# select relevant cells based on k-means clustering\n",
    "relevant_columns = ['Population', 'GDP']\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "clusters = kmeans.fit_predict(data[relevant_columns])\n",
    "relevant_data = data[clusters == 1]\n",
    "\n",
    "# select relevant cells based on linear regression\n",
    "regression = LinearRegression()\n",
    "X = data['Population'].values.reshape(-1, 1)\n",
    "y = data['GDP'].values.reshape(-1, 1)\n",
    "regression.fit(X, y)\n",
    "predicted_GDP = regression.predict(X)\n",
    "relevant_data['Predicted GDP'] = predicted_GDP\n",
    "relevant_columns.append('Predicted GDP')\n",
    "\n",
    "print(relevant_data[relevant_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f77fb4a-71cf-4b90-ab08-dfe2ad638a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [USA, China, India, Russia, Japan]\n"
     ]
    }
   ],
   "source": [
    "# In this example, I am selecting the relevant cells based on a t-test to determine if there is a significant difference between each country's data and the mean of all countries in the columns.\n",
    "# I have performed the t-test on each column, and have selected the columns with a p-value less than 0.05.\n",
    "\n",
    "# perform a t-test to determine the significant columns\n",
    "relevant_columns = []\n",
    "for column in data.columns:\n",
    "    country_data = data[column]\n",
    "    t_statistic, p_value = ttest_ind(country_data, data.mean())\n",
    "    if p_value < 0.05:\n",
    "        relevant_columns.append(column)\n",
    "\n",
    "print(data[relevant_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f8f0b-4558-4f28-80a2-e08467e58bda",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using unsupervised methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97fa54b-c1e6-49de-83da-910506f5bc8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "644305ac-5671-4cfd-bae7-c89e65f3f7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City: Cluster 1\n",
      "Los Angeles: Cluster 0\n",
      "Chicago: Cluster 0\n",
      "Houston: Cluster 0\n",
      "Phoenix: Cluster 0\n",
      "Philadelphia: Cluster 0\n",
      "San Antonio: Cluster 0\n",
      "San Diego: Cluster 0\n",
      "Dallas: Cluster 0\n",
      "San Jose: Cluster 0\n"
     ]
    }
   ],
   "source": [
    "table_data = [\n",
    "    ['City', 'Population', 'Area (km²)', 'GDP per capita (USD)', 'Elevation (meters)'],\n",
    "    ['New York City', '8,336,817', '468.9', '93,574', '10'],\n",
    "    ['Los Angeles', '3,979,576', '1,213.9', '62,820', '71'],\n",
    "    ['Chicago', '2,693,976', '606.1', '69,188', '182'],\n",
    "    ['Houston', '2,320,268', '1,651.1', '62,095', '18'],\n",
    "    ['Phoenix', '1,680,992', '1,338.3', '54,956', '331'],\n",
    "    ['Philadelphia', '1,584,064', '347.6', '60,116', '12'],\n",
    "    ['San Antonio', '1,547,253', '1,067.3', '47,419', '198'],\n",
    "    ['San Diego', '1,423,851', '372.4', '64,334', '22'],\n",
    "    ['Dallas', '1,343,573', '997.0', '61,347', '137'],\n",
    "    ['San Jose', '1,015,785', '467.2', '82,880', '27']\n",
    "]\n",
    "\n",
    "num_data = []\n",
    "for row in table_data[1:]:\n",
    "    num_row = []\n",
    "    for val in row[1:]:\n",
    "        val = val.replace(',', '')  # remove commas in numbers\n",
    "        try:\n",
    "            num_row.append(float(val))\n",
    "        except ValueError:\n",
    "            num_row.append(np.nan)\n",
    "    num_data.append(num_row)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2).fit(num_data)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "for i, row in enumerate(table_data[1:]):\n",
    "    print(f\"{row[0]}: Cluster {labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ca8ad-20e0-4e98-9385-ca943c68f876",
   "metadata": {},
   "source": [
    "### Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e838439-32ac-47ad-b322-79030e8f3243",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              PC1       PC2\n",
      "Country                    \n",
      "USA     -1.284715 -0.968521\n",
      "China    0.365595 -1.310261\n",
      "India    1.944905 -0.001206\n",
      "Russia   0.438567  1.353606\n",
      "Japan   -1.464353  0.926382\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({\n",
    "    'Country': ['USA', 'China', 'India', 'Russia', 'Japan'],\n",
    "    'Population': [331, 1393, 1380, 144, 126],\n",
    "    'GDP': [21428, 14732, 2955, 1678, 5068],\n",
    "    'Life Expectancy': [78.9, 76.9, 69.7, 72.3, 84.6]\n",
    "})\n",
    "\n",
    "data = data.set_index('Country')\n",
    "\n",
    "features = data.values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "\n",
    "selected_cells = pd.DataFrame(principal_components, index=data.index, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(selected_cells.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc08390-b1fb-4cf3-bd41-45337ee8388a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              PC1       PC2  Population Density  Development Index\n",
      "Country                                                           \n",
      "USA     -1.284715 -0.968521           -0.003881        -263.035035\n",
      "China    0.365595 -1.310261            0.000262        -251.011329\n",
      "India    1.944905 -0.001206            0.001409          -0.051114\n",
      "Russia   0.438567  1.353606            0.003046          31.415639\n",
      "Japan   -1.464353  0.926382           -0.011622          55.495316\n"
     ]
    }
   ],
   "source": [
    "# I have added some domain knowledge to interpret the results of the PCA.\n",
    "\n",
    "# Population Density: This column is calculated by dividing the first principal component (PC1) by the population of each country. \n",
    "# A measure of the population density of each country.\n",
    "\n",
    "# Development Index: This column is calculated by multiplying the second principal component (PC2) by the GDP of each country and dividing by the life expectancy. \n",
    "# A measure of the development index of each country.\n",
    "\n",
    "# add some domain knowledge to interpret the results\n",
    "selected_cells['Population Density'] = selected_cells['PC1'] / data['Population']\n",
    "selected_cells['Development Index'] = selected_cells['PC2'] * data['GDP'] / data['Life Expectancy']\n",
    "\n",
    "print(selected_cells.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9bdb166-62fa-46bb-a5b5-bcf6bfff8249",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              PC1       PC2  Population Density  Development Index  Cluster  \\\n",
      "Country                                                                       \n",
      "USA     -1.284715 -0.968521           -0.003881        -263.035035        1   \n",
      "China    0.365595 -1.310261            0.000262        -251.011329        0   \n",
      "India    1.944905 -0.001206            0.001409          -0.051114        0   \n",
      "Russia   0.438567  1.353606            0.003046          31.415639        0   \n",
      "Japan   -1.464353  0.926382           -0.011622          55.495316        1   \n",
      "\n",
      "         PC1 Residuals  GDP Correlation  GDP P-Value  \n",
      "Country                                               \n",
      "USA          -0.753917        -0.836548     0.077353  \n",
      "China        -0.743243        -0.836548     0.077353  \n",
      "India         0.856138        -0.836548     0.077353  \n",
      "Russia        1.258076        -0.836548     0.077353  \n",
      "Japan        -0.617053        -0.836548     0.077353  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lnet/work/people/mukherjee/own_code/venv_pytrch/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# In this example, I have added three additional techniques to provide insights into the relationships between the cells in the table:\n",
    "\n",
    "# Clustering: I have used k-means clustering to group the countries into two clusters based on their principal components. \n",
    "# I have added a new column to the resulting table (Cluster) to indicate which cluster each country belongs to.\n",
    "\n",
    "# Regression: I have used linear regression to fit the first principal component (PC1) to the population of each country.\n",
    "\n",
    "# Hypothesis testing: We have used Pearson's correlation coefficient to test the relationship between the GDP and the second principal component (PC2). \n",
    "# Two new columns to the resulting table (GDP Correlation and GDP P-Value) to show the strength and statistical significance of this relationship.\n",
    "\n",
    "# add clustering information\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(selected_cells[['PC1', 'PC2']])\n",
    "selected_cells['Cluster'] = clusters\n",
    "\n",
    "# add regression information\n",
    "reg = LinearRegression().fit(data[['Population']], selected_cells['PC1'])\n",
    "selected_cells['PC1 Residuals'] = selected_cells['PC1'] - reg.predict(data[['Population']])\n",
    "r, p = pearsonr(data['GDP'], selected_cells['PC2'])\n",
    "selected_cells['GDP Correlation'] = r\n",
    "selected_cells['GDP P-Value'] = p\n",
    "\n",
    "print(selected_cells.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abfc39-d023-4225-960b-f9478ca1c04a",
   "metadata": {},
   "source": [
    "### \"cell interestingness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35eb7e7d-c62b-4ff4-ae4c-5289d262dde6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monotonicity breaks at row 2 in column column2\n",
      "Monotonicity breaks at row 2 in column column3\n"
     ]
    }
   ],
   "source": [
    "# tthe values at which the monotonicity breaks. \n",
    "# For instance, some column values are descreasing, but starting from a certain row they increase.\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'column1': [1, 2, 3, 4, 5],\n",
    "    'column2': [10, 9, 8, 11, 12],\n",
    "    'column3': [100, 200, 300, 250, 150]\n",
    "})\n",
    "\n",
    "for column in df.columns:\n",
    "    for i in range(len(df) - 1):\n",
    "        if (df[column].iloc[i] < df[column].iloc[i+1] and\n",
    "                i > 0 and df[column].iloc[i-1] >= df[column].iloc[i]):\n",
    "            print(f\"Monotonicity breaks at row {i} in column {column}\")\n",
    "        elif (df[column].iloc[i] > df[column].iloc[i+1] and\n",
    "                i > 0 and df[column].iloc[i-1] <= df[column].iloc[i]):\n",
    "            print(f\"Monotonicity breaks at row {i} in column {column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57408480-7b8d-4bb4-ac9c-5ba80994065f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
